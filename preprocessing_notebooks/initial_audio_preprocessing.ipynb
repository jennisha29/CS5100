{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536557e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, glob\n",
    "import librosa\n",
    "import librosa.display\n",
    "import IPython\n",
    "from IPython.display import Audio\n",
    "from IPython.display import Image\n",
    "import warnings; warnings.filterwarnings('ignore') #matplot lib complains about librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2b6964",
   "metadata": {},
   "outputs": [],
   "source": [
    "#google colab has an old version of librosa with missing mel spectrogram args (for MFCC); upgrade to current\n",
    "!pip install -U librosa\n",
    "\n",
    "# needed to import dataset from google drive into colab \n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/gdrive\")\n",
    "\n",
    "# copy RAVDESS dataset from gdrive and unzip\n",
    "!cp '/content/gdrive/My Drive/DL/RAVDESS.zip' .\n",
    "!unzip -q RAVDESS.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4fdac3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define features\n",
    "# RAVDESS native sample rate is 48k\n",
    "sample_rate = 48000\n",
    "\n",
    "# Mel Spectrograms are not directly used as a feature in this model\n",
    "# Mel Spectrograms are used in calculating MFCCs, which are a higher-level representation of pitch transition\n",
    "# MFCCs work better - left the mel spectrogram function here in case anyone wants to experiment\n",
    "def feature_melspectrogram(\n",
    "    waveform, \n",
    "    sample_rate,\n",
    "    fft = 1024,\n",
    "    winlen = 512,\n",
    "    window='hamming',\n",
    "    hop=256,\n",
    "    mels=128,\n",
    "    ):\n",
    "    \n",
    "    # Produce the mel spectrogram for all STFT frames and get the mean of each column of the resulting matrix to create a feature array\n",
    "    # Using 8khz as upper frequency bound should be enough for most speech classification tasks\n",
    "    melspectrogram = librosa.feature.melspectrogram(\n",
    "        y=waveform, \n",
    "        sr=sample_rate, \n",
    "        n_fft=fft, \n",
    "        win_length=winlen, \n",
    "        window=window, \n",
    "        hop_length=hop, \n",
    "        n_mels=mels, \n",
    "        fmax=sample_rate/2)\n",
    "    \n",
    "    # convert from power (amplitude**2) to decibels\n",
    "    # necessary for network to learn - doesn't converge with raw power spectrograms \n",
    "    melspectrogram = librosa.power_to_db(melspectrogram, ref=np.max)\n",
    "    \n",
    "    return melspectrogram\n",
    "\n",
    "def feature_mfcc(\n",
    "    waveform, \n",
    "    sample_rate,\n",
    "    n_mfcc = 40,\n",
    "    fft = 1024,\n",
    "    winlen = 512,\n",
    "    window='hamming',\n",
    "    #hop=256, # increases # of time steps; was not helpful\n",
    "    mels=128\n",
    "    ):\n",
    "\n",
    "    # Compute the MFCCs for all STFT frames \n",
    "    # 40 mel filterbanks (n_mfcc) = 40 coefficients\n",
    "    mfc_coefficients=librosa.feature.mfcc(\n",
    "        y=waveform, \n",
    "        sr=sample_rate, \n",
    "        n_mfcc=n_mfcc,\n",
    "        n_fft=fft, \n",
    "        win_length=winlen, \n",
    "        window=window, \n",
    "        #hop_length=hop, \n",
    "        n_mels=mels, \n",
    "        fmax=sample_rate/2\n",
    "        ) \n",
    "\n",
    "    return mfc_coefficients\n",
    "\n",
    "def get_features(waveforms, features, samplerate):\n",
    "\n",
    "    # initialize counter to track progress\n",
    "    file_count = 0\n",
    "\n",
    "    # process each waveform individually to get its MFCCs\n",
    "    for waveform in waveforms:\n",
    "        mfccs = feature_mfcc(waveform, sample_rate)\n",
    "        features.append(mfccs)\n",
    "        file_count += 1\n",
    "        # print progress \n",
    "        print('\\r'+f' Processed {file_count}/{len(waveforms)} waveforms',end='')\n",
    "    \n",
    "    # return all features from list of waveforms\n",
    "    return features\n",
    "\n",
    "def get_waveforms(file):\n",
    "    \n",
    "    # load an individual sample audio file\n",
    "    # read the full 3 seconds of the file, cut off the first 0.5s of silence; native sample rate = 48k\n",
    "    # don't need to store the sample rate that librosa.load returns\n",
    "    waveform, _ = librosa.load(file, duration=3, offset=0.5, sr=sample_rate)\n",
    "    \n",
    "    # make sure waveform vectors are homogenous by defining explicitly\n",
    "    waveform_homo = np.zeros((int(sample_rate*3,)))\n",
    "    waveform_homo[:len(waveform)] = waveform\n",
    "    \n",
    "    # return a single file's waveform                                      \n",
    "    return waveform_homo\n",
    "    \n",
    "# RAVDESS dataset emotions\n",
    "# shift emotions left to be 0 indexed for PyTorch\n",
    "emotions_dict ={\n",
    "    '0':'surprised',\n",
    "    '1':'neutral',\n",
    "    '2':'calm',\n",
    "    '3':'happy',\n",
    "    '4':'sad',\n",
    "    '5':'angry',\n",
    "    '6':'fearful',\n",
    "    '7':'disgust'\n",
    "}\n",
    "\n",
    "# Additional attributes from RAVDESS to play with\n",
    "emotion_attributes = {\n",
    "    '01': 'normal',\n",
    "    '02': 'strong'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e912eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load Data and Extract Features\n",
    "\n",
    "\n",
    "##We process each file in the dataset and extract its features.\n",
    "\n",
    "##We return the waveforms and the labels (from the file names of the RAVDESS audio samples). We return the raw waveforms because we're going to do some extra processing.\n",
    "\n",
    "## path to data for glob\n",
    "data_path = 'RAVDESS dataset/Actor_*/*.wav'\n",
    "\n",
    "def load_data():\n",
    "    # features and labels\n",
    "    emotions = []\n",
    "    # raw waveforms to augment later\n",
    "    waveforms = []\n",
    "    # extra labels\n",
    "    intensities, genders = [],[]\n",
    "    # progress counter\n",
    "    file_count = 0\n",
    "    for file in glob.glob(data_path):\n",
    "        # get file name with labels\n",
    "        file_name = os.path.basename(file)\n",
    "        \n",
    "        # get emotion label from the sample's file\n",
    "        emotion = int(file_name.split(\"-\")[2])\n",
    "\n",
    "        #  move surprise to 0 for cleaner behaviour with PyTorch/0-indexing\n",
    "        if emotion == 8: emotion = 0 # surprise is now at 0 index; other emotion indeces unchanged\n",
    "\n",
    "        # can convert emotion label to emotion string if desired, but\n",
    "        # training on number is better; better convert to emotion string after predictions are ready\n",
    "        # emotion = emotions_dict[str(emotion)]\n",
    "        \n",
    "        # get other labels we might want\n",
    "        intensity = emotion_attributes[file_name.split(\"-\")[3]]\n",
    "        # even actors are female, odd are male\n",
    "        if (int((file_name.split(\"-\")[6]).split(\".\")[0]))%2==0: \n",
    "            gender = 'female' \n",
    "        else: \n",
    "            gender = 'male'\n",
    "            \n",
    "        # get waveform from the sample\n",
    "        waveform = get_waveforms(file)\n",
    "        \n",
    "        # store waveforms and labels\n",
    "        waveforms.append(waveform)\n",
    "        emotions.append(emotion)\n",
    "        intensities.append(intensity) # store intensity in case we wish to predict\n",
    "        genders.append(gender) # store gender in case we wish to predict \n",
    "        \n",
    "        file_count += 1\n",
    "        # keep track of data loader's progress\n",
    "        print('\\r'+f' Processed {file_count}/{1440} audio samples',end='')\n",
    "        \n",
    "    return waveforms, emotions, intensities, genders\n",
    "\n",
    "# load data \n",
    "# init explicitly to prevent data leakage from past sessions, since load_data() appends\n",
    "waveforms, emotions, intensities, genders = [],[],[],[]\n",
    "waveforms, emotions, intensities, genders = load_data()\n",
    "\n",
    "\"\"\"## Check extracted audio waveforms and labels:\"\"\"\n",
    "\n",
    "print(f'Waveforms set: {len(waveforms)} samples')\n",
    "# we have 1440 waveforms but we need to know their length too; should be 3 sec * 48k = 144k\n",
    "print(f'Waveform signal length: {len(waveforms[0])}')\n",
    "print(f'Emotions set: {len(emotions)} sample labels')\n",
    "\n",
    "\"\"\"Looks good. 1440 samples and 1440 labels in total.\n",
    "\n",
    "**Waveforms are 144k long because 3 seconds * 48k sample rate = 144k length array representing the 3 second audio snippet.**\n",
    "\n",
    "## Split into Train/Validation/Test Sets\n",
    "We'll use an 80/10/10 train/validation/test split to maximize training data and keep a reasonable validation/test set. \n",
    "\n",
    "**We're splitting waveforms so we can process train/validation/test waveforms separately and avoid data leakage.** \n",
    "\n",
    "**Have to take care to split the sets proportionally w.r.t. emotion.**\n",
    "\"\"\"\n",
    "\n",
    "# create storage for train, validation, test sets and their indices\n",
    "train_set,valid_set,test_set = [],[],[]\n",
    "X_train,X_valid,X_test = [],[],[]\n",
    "y_train,y_valid,y_test = [],[],[]\n",
    "\n",
    "# convert waveforms to array for processing\n",
    "waveforms = np.array(waveforms)\n",
    "\n",
    "# process each emotion separately to make sure we builf balanced train/valid/test sets \n",
    "for emotion_num in range(len(emotions_dict)):\n",
    "        \n",
    "    # find all indices of a single unique emotion\n",
    "    emotion_indices = [index for index, emotion in enumerate(emotions) if emotion==emotion_num]\n",
    "\n",
    "    # seed for reproducibility \n",
    "    np.random.seed(69)\n",
    "    # shuffle indicies \n",
    "    emotion_indices = np.random.permutation(emotion_indices)\n",
    "\n",
    "    # store dim (length) of the emotion list to make indices\n",
    "    dim = len(emotion_indices)\n",
    "\n",
    "    # store indices of training, validation and test sets in 80/10/10 proportion\n",
    "    # train set is first 80%\n",
    "    train_indices = emotion_indices[:int(0.8*dim)]\n",
    "    # validation set is next 10% (between 80% and 90%)\n",
    "    valid_indices = emotion_indices[int(0.8*dim):int(0.9*dim)]\n",
    "    # test set is last 10% (between 90% - end/100%)\n",
    "    test_indices = emotion_indices[int(0.9*dim):]\n",
    "\n",
    "    # create train waveforms/labels sets\n",
    "    X_train.append(waveforms[train_indices,:])\n",
    "    y_train.append(np.array([emotion_num]*len(train_indices),dtype=np.int32))\n",
    "    # create validation waveforms/labels sets\n",
    "    X_valid.append(waveforms[valid_indices,:])\n",
    "    y_valid.append(np.array([emotion_num]*len(valid_indices),dtype=np.int32))\n",
    "    # create test waveforms/labels sets\n",
    "    X_test.append(waveforms[test_indices,:])\n",
    "    y_test.append(np.array([emotion_num]*len(test_indices),dtype=np.int32))\n",
    "\n",
    "    # store indices for each emotion set to verify uniqueness between sets \n",
    "    train_set.append(train_indices)\n",
    "    valid_set.append(valid_indices)\n",
    "    test_set.append(test_indices)\n",
    "\n",
    "# concatenate, in order, all waveforms back into one array \n",
    "X_train = np.concatenate(X_train,axis=0)\n",
    "X_valid = np.concatenate(X_valid,axis=0)\n",
    "X_test = np.concatenate(X_test,axis=0)\n",
    "\n",
    "# concatenate, in order, all emotions back into one array \n",
    "y_train = np.concatenate(y_train,axis=0)\n",
    "y_valid = np.concatenate(y_valid,axis=0)\n",
    "y_test = np.concatenate(y_test,axis=0)\n",
    "\n",
    "# combine and store indices for all emotions' train, validation, test sets to verify uniqueness of sets\n",
    "train_set = np.concatenate(train_set,axis=0)\n",
    "valid_set = np.concatenate(valid_set,axis=0)\n",
    "test_set = np.concatenate(test_set,axis=0)\n",
    "\n",
    "# check shape of each set\n",
    "print(f'X_train:{X_train.shape}, y_train:{y_train.shape}')\n",
    "print(f'X_valid:{X_valid.shape}, y_valid:{y_valid.shape}')\n",
    "print(f'X_test:{X_test.shape}, y_test:{y_test.shape}')\n",
    "\n",
    "# make sure train, validation, test sets have no overlap/are unique\n",
    "# get all unique indices across all sets and how many times each index appears (count)\n",
    "uniques, count = np.unique(np.concatenate([train_set,test_set,valid_set],axis=0), return_counts=True)\n",
    "\n",
    "# if each index appears just once, and we have 1440 such unique indices, then all sets are unique\n",
    "if sum(count==1) == len(emotions):\n",
    "    print(f'\\nSets are unique: {sum(count==1)} samples out of {len(emotions)} are unique')\n",
    "else:\n",
    "    print(f'\\nSets are NOT unique: {sum(count==1)} samples out of {len(emotions)} are unique')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e6f186",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Augmenting the Data with AWGN - Additive White Gaussian Noise\n",
    "\n",
    "### Motivation\n",
    "\n",
    "##Since our dataset is small, it is prone to overfitting - especially with highly parameterized deep neural net models\n",
    "##such as the one we aim to build in this notebook. As such, we're going to want to augment our data. Generating more real samples will be immensely difficult. Instead, we can add white noise to the audio signals - not only to mask the effect of random noise present in the training set - but also **to create pseudo-new training samples and offset the impact of noise intrinsic to the dataset.** \n",
    "\n",
    "##In addition, the RAVDESS dataset is extremely clean - we will likely want to make predictions on noisy, real-world data - yet another reason to augment the training data.\n",
    "\n",
    "##We're going to use Additive White Gaussian Noise (AWGN). It's Additive because we're adding it to the source audio signal,\n",
    "##**it's Gaussian because the noise vector will be sampled from a normal distribution and have a time average of zero (zero-mean), and it's white because after a whitening transformation the noise will add power to the audio signal uniformly across the frequency distribution.**\n",
    "\n",
    "##We need a good balance of noise - too little will be useless, and too much will make it too difficult for the network to learn from the training data. **Note that this is just for training - we would _not_ need to add AWGN to real-world data on which we make predictions** (although we could). \n",
    "\n",
    "### Math\n",
    "##The key parameters in AWGN are the signal to noise ratio (SNR), defining the magnitude of the noise added w.r.t. the audio signal. We parameterize AWGN with the minimum and maximize SNR so we can pick a random SNR to use in augmenting each sample's waveform.\n",
    "\n",
    "##We need to constrain covariance to make it true AWGN. **We make a zero-mean vector of Gaussian noises (np.random.normal) that are statistically dependent. We need to apply a [whitening transformation](https://en.wikipedia.org/wiki/Whitening_transformation)**, a linear transformation taking a vector of random normal (Gaussian) variables with a known covariance matrix and mapping it to a new vector whose covariance is the identity matrix, i.e. the vector is now perfectly uncorrelated with a diaganol covariance matrix, each point of noise having variance == stdev == 1. **The whitening transformation by definition transforms a vector into a white noise vector.**\n",
    "\n",
    "##We're going to add the AWGN augmented waveforms as new samples to our dataset. **Since we generate AWGN which is random for each and every sample - random random noise - we can add multiples of our noise-augmented dataset. I'll add 2 extra identical, randomly noisy datasets with 1440 samples each to get a dataset with 1440 native + 1440x2 == 4320 noisy samples.**\n",
    "\n",
    "\n",
    "def awgn_augmentation(waveform, multiples=2, bits=16, snr_min=15, snr_max=30): \n",
    "    \n",
    "    # get length of waveform (should be 3*48k = 144k)\n",
    "    wave_len = len(waveform)\n",
    "    \n",
    "    # Generate normally distributed (Gaussian) noises\n",
    "    # one for each waveform and multiple (i.e. wave_len*multiples noises)\n",
    "    noise = np.random.normal(size=(multiples, wave_len))\n",
    "    \n",
    "    # Normalize waveform and noise\n",
    "    norm_constant = 2.0**(bits-1)\n",
    "    norm_wave = waveform / norm_constant\n",
    "    norm_noise = noise / norm_constant\n",
    "    \n",
    "    # Compute power of waveform and power of noise\n",
    "    signal_power = np.sum(norm_wave ** 2) / wave_len\n",
    "    noise_power = np.sum(norm_noise ** 2, axis=1) / wave_len\n",
    "    \n",
    "    # Choose random SNR in decibels in range [15,30]\n",
    "    snr = np.random.randint(snr_min, snr_max)\n",
    "    \n",
    "    # Apply whitening transformation: make the Gaussian noise into Gaussian white noise\n",
    "    # Compute the covariance matrix used to whiten each noise \n",
    "    # actual SNR = signal/noise (power)\n",
    "    # actual noise power = 10**(-snr/10)\n",
    "    covariance = np.sqrt((signal_power / noise_power) * 10 ** (- snr / 10))\n",
    "    # Get covariance matrix with dim: (144000, 2) so we can transform 2 noises: dim (2, 144000)\n",
    "    covariance = np.ones((wave_len, multiples)) * covariance\n",
    "\n",
    "    # Since covariance and noise are arrays, * is the haddamard product \n",
    "    # Take Haddamard product of covariance and noise to generate white noise\n",
    "    multiple_augmented_waveforms = waveform + covariance.T * noise\n",
    "    \n",
    "    return multiple_augmented_waveforms\n",
    "\n",
    "def augment_waveforms(waveforms, features, emotions, multiples):\n",
    "    # keep track of how many waveforms we've processed so we can add correct emotion label in the same order\n",
    "    emotion_count = 0\n",
    "    # keep track of how many augmented samples we've added\n",
    "    added_count = 0\n",
    "    # convert emotion array to list for more efficient appending\n",
    "    emotions = emotions.tolist()\n",
    "\n",
    "    for waveform in waveforms:\n",
    "\n",
    "        # Generate 2 augmented multiples of the dataset, i.e. 1440 native + 1440*2 noisy = 4320 samples total\n",
    "        augmented_waveforms = awgn_augmentation(waveform, multiples=multiples)\n",
    "\n",
    "        # compute spectrogram for each of 2 augmented waveforms\n",
    "        for augmented_waveform in augmented_waveforms:\n",
    "\n",
    "            # Compute MFCCs over augmented waveforms\n",
    "            augmented_mfcc = feature_mfcc(augmented_waveform, sample_rate=sample_rate)\n",
    "\n",
    "            # append the augmented spectrogram to the rest of the native data\n",
    "            features.append(augmented_mfcc)\n",
    "            emotions.append(emotions[emotion_count])\n",
    "\n",
    "            # keep track of new augmented samples\n",
    "            added_count += 1\n",
    "\n",
    "            # check progress\n",
    "            print('\\r'+f'Processed {emotion_count + 1}/{len(waveforms)} waveforms for {added_count}/{len(waveforms)*multiples} new augmented samples',end='')\n",
    "\n",
    "        # keep track of the emotion labels to append in order\n",
    "        emotion_count += 1\n",
    "        \n",
    "        # store augmented waveforms to check their shape\n",
    "        augmented_waveforms_temp.append(augmented_waveforms)\n",
    "    \n",
    "    return features, emotions\n",
    "\n",
    "## Compute AWGN-augmented features and add to the rest of the dataset\n",
    "\n",
    "# store augmented waveforms to verify their shape and random-ness\n",
    "augmented_waveforms_temp = []\n",
    "\n",
    "# specify multiples of our dataset to add as augmented data\n",
    "multiples = 2\n",
    "\n",
    "print('Train waveforms:') # augment waveforms of training set\n",
    "features_train , y_train = augment_waveforms(X_train, features_train, y_train, multiples)\n",
    "\n",
    "print('\\n\\nValidation waveforms:') # augment waveforms of validation set\n",
    "features_valid, y_valid = augment_waveforms(X_valid, features_valid, y_valid, multiples)\n",
    "\n",
    "print('\\n\\nTest waveforms:') # augment waveforms of test set \n",
    "features_test, y_test = augment_waveforms(X_test, features_test, y_test, multiples)\n",
    "\n",
    "# Check new shape of extracted features and data:\n",
    "print(f'\\n\\nNative + Augmented Features set: {len(features_train)+len(features_test)+len(features_valid)} total, {len(features_train)} train, {len(features_valid)} validation, {len(features_test)} test samples')\n",
    "print(f'{len(y_train)} training sample labels, {len(y_valid)} validation sample labels, {len(y_test)} test sample labels')\n",
    "print(f'Features (MFCC matrix) shape: {len(features_train[0])} mel frequency coefficients x {len(features_train[0][1])} time steps')\n",
    "\n",
    "\"\"\"### Check Augmented Waveforms:\"\"\"\n",
    "\n",
    "# pick a random waveform, but same one from native and augmented set for easier comparison\n",
    "plt.figure(figsize=(15,4))\n",
    "plt.subplot(1, 2, 1)\n",
    "librosa.display.waveplot(waveforms[1], sr=sample_rate)\n",
    "plt.title('Native')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "librosa.display.waveplot(augmented_waveforms_temp[1], sr=sample_rate)\n",
    "plt.title('AWGN Augmented')\n",
    "plt.show()\n",
    "\n",
    "\"\"\"Looks noisy alright. Noise is clearly visible in otherwise-silent regions of the waveform.\n",
    "\n",
    "**Note that augmentation was only done after splitting data into train, validation, and test sets - and we processed each set separately.**\n",
    "\n",
    "**When we augmented the data before splitting it, test and validation data leaked into the training set, giving a 97% test accuracy.**\n",
    "\n",
    "## Format Data into Tensor Ready 4D Arrays\n",
    "We don't have a colour channel in our MFCC feature array of dim (#samples, #MFC coefficients, time steps). **We have an analog of a black and white image: instead of 3 colour channels, we have 1 signal intensity channel: magnitude of each of 40 mel frequency coefficients at time t.**\n",
    "\n",
    "**We need an input channel dim to expand to output channels using CNN filters. We create a dummy channel dim to expand features into 2D-CNN-ready 4D tensor format: N x C x H x W.**\n",
    "\"\"\"\n",
    "\n",
    "# need to make dummy input channel for CNN input feature tensor\n",
    "X_train = np.expand_dims(features_train,1)\n",
    "X_valid = np.expand_dims(features_valid, 1)\n",
    "X_test = np.expand_dims(features_test,1)\n",
    "\n",
    "# convert emotion labels from list back to numpy arrays for PyTorch to work with \n",
    "y_train = np.array(y_train)\n",
    "y_valid = np.array(y_valid)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "# confiorm that we have tensor-ready 4D data array\n",
    "# should print (batch, channel, width, height) == (4320, 1, 128, 282) when multiples==2\n",
    "print(f'Shape of 4D feature array for input tensor: {X_train.shape} train, {X_valid.shape} validation, {X_test.shape} test')\n",
    "print(f'Shape of emotion labels: {y_train.shape} train, {y_valid.shape} validation, {y_test.shape} test')\n",
    "\n",
    "# free up some RAM - no longer need full feature set or any waveforms \n",
    "del features_train, features_valid, features_test, waveforms, augmented_waveforms_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cea3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Feature Scaling\n",
    "#Scaling will drastically decrease the length of time the model needs to train to convergence - it will have easier computations to perform on smaller magnitudes. **For reference, scaling reduces the time to convergence from about 500 to 200 epochs for this model.**\n",
    "\n",
    "#Standard Scaling makes the most sense because we have features whose target distribution we don't know.** When I performed classification on this dataset with an MLP classifier standard scaling was best across a variety of conditions and features.\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "#### Scale the training data ####\n",
    "# store shape so we can transform it back \n",
    "N,C,H,W = X_train.shape\n",
    "# Reshape to 1D because StandardScaler operates on a 1D array\n",
    "# tell numpy to infer shape of 1D array with '-1' argument\n",
    "X_train = np.reshape(X_train, (N,-1)) \n",
    "X_train = scaler.fit_transform(X_train)\n",
    "# Transform back to NxCxHxW 4D tensor format\n",
    "X_train = np.reshape(X_train, (N,C,H,W))\n",
    "\n",
    "##### Scale the validation set ####\n",
    "N,C,H,W = X_valid.shape\n",
    "X_valid = np.reshape(X_valid, (N,-1))\n",
    "X_valid = scaler.transform(X_valid)\n",
    "X_valid = np.reshape(X_valid, (N,C,H,W))\n",
    "\n",
    "#### Scale the test set ####\n",
    "N,C,H,W = X_test.shape\n",
    "X_test = np.reshape(X_test, (N,-1))\n",
    "X_test = scaler.transform(X_test)\n",
    "X_test = np.reshape(X_test, (N,C,H,W))\n",
    "\n",
    "# check shape of each set again\n",
    "print(f'X_train scaled:{X_train.shape}, y_train:{y_train.shape}')\n",
    "print(f'X_valid scaled:{X_valid.shape}, y_valid:{y_valid.shape}')\n",
    "print(f'X_test scaled:{X_test.shape}, y_test:{y_test.shape}')\n",
    "\n",
    "\"\"\"### Save and Reload Data as NumPy Arrays \n",
    "We can save the training/validation/test data as numpy arrays to enable faster loading in case the notebook kernel crashes / google colab runtime crashes / any number of reasons the training data might be cleared from memory. This is much faster than loading 1440 files and computing their features again - not to mention augmented features.\n",
    "\"\"\"\n",
    "\n",
    "###### SAVE #########\n",
    "# choose save file name \n",
    "filename = 'features+labels.npy'\n",
    "\n",
    "# open file in write mode and write data\n",
    "with open(filename, 'wb') as f:\n",
    "    np.save(f, X_train)\n",
    "    np.save(f, X_valid)\n",
    "    np.save(f, X_test)\n",
    "    np.save(f, y_train)\n",
    "    np.save(f, y_valid)\n",
    "    np.save(f, y_test)\n",
    "\n",
    "print(f'Features and labels saved to {filename}')\n",
    "\n",
    "##### LOAD #########\n",
    "# choose load file name \n",
    "filename = 'features+labels.npy'\n",
    "\n",
    "# open file in read mode and read data \n",
    "with open(filename, 'rb') as f:\n",
    "    X_train = np.load(f)\n",
    "    X_valid = np.load(f)\n",
    "    X_test = np.load(f)\n",
    "    y_train = np.load(f)\n",
    "    y_valid = np.load(f)\n",
    "    y_test = np.load(f)\n",
    "\n",
    "# Check that we've recovered the right data\n",
    "print(f'X_train:{X_train.shape}, y_train:{y_train.shape}')\n",
    "print(f'X_valid:{X_valid.shape}, y_valid:{y_valid.shape}')\n",
    "print(f'X_test:{X_test.shape}, y_test:{y_test.shape}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
